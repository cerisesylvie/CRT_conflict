{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langdetect import detect\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.consiglio.vda.it/app/oggettidelconsiglio/dettaglio?pk_documento={}&versione=R\"\n",
    "\n",
    "OUTPUT_FOLDER = \"./downloads\"\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "#funzione per scraperare e salvare i resoconti dal sito del consiglio Valle\n",
    "def scrape_and_save(doc_id):\n",
    "    url = BASE_URL.format(doc_id)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Documento {doc_id} non trovato (HTTP {response.status_code}).\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    page_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    output_file = os.path.join(OUTPUT_FOLDER, f\"{doc_id}.txt\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(page_text)\n",
    "    \n",
    "    print(f\"Documento {doc_id} salvato in: {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "\n",
    "def main(start_id, end_id):\n",
    "    csv_data = []\n",
    "    print(f\"Inizio lo scraping per i documenti dal {start_id} al {end_id}...\")\n",
    "\n",
    "    for doc_id in range(start_id, end_id + 1):\n",
    "        print(f\"Processo il documento {doc_id}...\")\n",
    "        file_path = scrape_and_save(doc_id)\n",
    "\n",
    "        if file_path:\n",
    "            csv_data.append({\"ID_file\": doc_id, \"path_src\": file_path})\n",
    "\n",
    "        # Pausa per evitare di sovraccaricare il server (ad esempio 2-5 secondi)\n",
    "        time.sleep(2 + (3 * doc_id % 5))  # Variamo la pausa per evitare un pattern troppo prevedibile\n",
    "\n",
    "    # Salvataggio dei risultati nel file CSV\n",
    "    csv_file = \"csv_paths.csv\"\n",
    "    print(f\"Salvataggio dei risultati nel file CSV: {csv_file}\")\n",
    "    with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"ID_file\", \"path_src\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_data)\n",
    "\n",
    "    print(\"Completato! Tutti i documenti salvati nella cartella:\", OUTPUT_FOLDER)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(start_id=47950, end_id=48020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione per pulire i file txt dalle parti inutili presenti nella pagina web del consiglio Valle\n",
    "def first_clean(target_folder, keyword, mid_strings, end_keyword, csv_paths):\n",
    "    df = pd.read_csv(csv_paths)\n",
    "\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "\n",
    "    df[\"path_clean\"] = \"\"\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        source_path = row[\"path_src\"]\n",
    "        if source_path.endswith(\".txt\"):\n",
    "            filename = os.path.basename(source_path)\n",
    "            target_path = os.path.join(target_folder, filename)\n",
    "\n",
    "            df.at[idx, \"path_clean\"] = target_path\n",
    "\n",
    "            with open(source_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "\n",
    "            if keyword in content:\n",
    "                content = keyword + content.split(keyword, 1)[1] \n",
    "\n",
    "            for mid_string in mid_strings:\n",
    "                content = re.sub(re.escape(mid_string), \"\", content) \n",
    "\n",
    "            if end_keyword in content:\n",
    "                content = content.split(end_keyword, 1)[0]  \n",
    "\n",
    "            with open(target_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(content)\n",
    "\n",
    "    df.to_csv(csv_paths, index=False)\n",
    "\n",
    "#funzione per estrarre informazioni dai file txt: oggetto, legislatura e classificazione\n",
    "def first_info(pattern1, pattern2, pattern3, csv_paths):\n",
    "    df_paths = pd.read_csv(csv_paths)\n",
    "\n",
    "    df_paths['language'] = \"\"\n",
    "    df_paths['object'] = \"\"\n",
    "    df_paths['legislature'] = \"\"\n",
    "    df_paths['class'] = \"\"\n",
    "\n",
    "    for idx, row in df_paths.iterrows():\n",
    "        source_path = row[\"path_clean\"]\n",
    "        if source_path.endswith(\".txt\"):\n",
    "            filename = os.path.basename(source_path)\n",
    "            filenum = int(os.path.splitext(filename)[0])\n",
    "\n",
    "            with open(source_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "\n",
    "            match1 = re.search(pattern1, content)\n",
    "\n",
    "            if match1:\n",
    "                stringa = match1.group(1).strip()\n",
    "                ogg, leg = stringa.split(\"/\", 1)\n",
    "\n",
    "                df_paths.loc[df_paths['ID_file'] == filenum, 'object'] = ogg\n",
    "                df_paths.loc[df_paths['ID_file'] == filenum, 'legislature'] = leg\n",
    "\n",
    "            match2 = re.search(pattern2, content, re.DOTALL)\n",
    "            \n",
    "            if match2:\n",
    "                classe = match2.group(1).strip()\n",
    "                classe = classe.replace(\"\\n\", \", \").strip()\n",
    "                df_paths.loc[df_paths['ID_file'] == filenum, 'class'] = classe\n",
    "\n",
    "            try:\n",
    "                lang = detect(content)\n",
    "                if lang == 'fr':\n",
    "                    df_paths.loc[df_paths['ID_file'] == filenum, 'language'] = 'fr'\n",
    "                elif lang == 'it':\n",
    "                    df_paths.loc[df_paths['ID_file'] == filenum, 'language'] = 'it'\n",
    "                else:\n",
    "                    df_paths.loc[df_paths['ID_file'] == filenum, 'language'] = 'other'\n",
    "            except:\n",
    "                df_paths.loc[df_paths['ID_file'] == filenum, 'language'] = 'error'\n",
    "\n",
    "    df_paths.to_csv(csv_paths, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_folder = \"./clean\"\n",
    "\n",
    "csv_paths = \"./csv_paths.csv\"\n",
    "csv_details = \"./csv_details.csv\"\n",
    "csv_cons = \"./csv_cons.csv\"\n",
    "csv_chunks = \"./csv_chunks.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"Classificazione\"\n",
    "mid_strings = [\"Precedente\", \"Successivo\", \"Resoconto integrale del dibattito dell'aula. I documenti allegati sono reperibili nel link \\\"iter atto\\\".\"]\n",
    "end_keyword = \"Informativa cookies\"\n",
    "\n",
    "first_clean(target_folder, keyword, mid_strings, end_keyword, csv_paths)\n",
    "\n",
    "pattern1 = r'(?:OGGETTO N\\.|OBJET NÂ°)(.*?)\\s*-'\n",
    "pattern2 = r'Classificazione\\s*(.*?)\\s*Oggetto'\n",
    "pattern3 = r'object'\n",
    "\n",
    "first_info(pattern1, pattern2, pattern3, csv_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione per stabilire i nomi dei consiglieri presenti nella legislatura presa in considerazione\n",
    "def define_names(csv_cons, leg):\n",
    "    df_cons = pd.read_csv(csv_cons)\n",
    "    names = []\n",
    "\n",
    "    print(f\"Filtrando per legislatura: {leg}\")\n",
    "    filtered_cons = df_cons[df_cons['legislature'].str.contains(leg, case=False, na=False)]\n",
    "    \n",
    "    surname_counts = filtered_cons['surname'].value_counts()\n",
    "\n",
    "    for idx, row in filtered_cons.iterrows():\n",
    "        surname = row[\"surname\"]\n",
    "        name = row[\"name\"]\n",
    "\n",
    "        if surname_counts[surname] > 1:\n",
    "            names.append(f\"{surname} {name[0]}.\")\n",
    "        else:\n",
    "            names.append(surname)\n",
    "\n",
    "    print(f\"Numero di nomi trovati per la legislatura {leg}: {len(names)}\")\n",
    "    return filtered_cons, names\n",
    "\n",
    "#funzione per isolare i singoli interventi, specificando chi interviene (con relativi dettagli) e il contenuto dell'intervento\n",
    "def isolate_chunk(csv_paths, csv_cons):\n",
    "    df_paths = pd.read_csv(csv_paths)\n",
    "    chunk_list = []\n",
    "\n",
    "    for idx, row in df_paths.iterrows():\n",
    "        leg = row[\"legislature\"]\n",
    "        language = row[\"language\"]\n",
    "        \n",
    "        print(f\"Processando file {row['path_clean']} per la legislatura {leg}...\")\n",
    "        \n",
    "        try:\n",
    "            df_cons, list_cons = define_names(csv_cons, leg)\n",
    "\n",
    "            with open(row[\"path_clean\"], 'r') as f:\n",
    "                text = f.read()\n",
    "\n",
    "                all_matches = []\n",
    "                for name in list_cons:\n",
    "                    pattern = r\"(?i)\" + re.escape(name) + r\"\\s?\\([^\\)]*\\)\\s?-\"\n",
    "                    matches = list(re.finditer(pattern, text))\n",
    "                    all_matches.extend(matches)\n",
    "\n",
    "                president_pattern = r\"(?i)Presidente\\s?-\"\n",
    "                president_matches = list(re.finditer(president_pattern, text))\n",
    "                all_matches.extend(president_matches)\n",
    "\n",
    "                all_matches.sort(key=lambda match: match.start())\n",
    "\n",
    "                president_data = None\n",
    "                if all_matches:\n",
    "                    first_match = all_matches[0]\n",
    "                    if re.match(president_pattern, first_match.group(0), re.IGNORECASE):\n",
    "                        president_data = None\n",
    "                    else:\n",
    "                        surname = first_match.group(0)\n",
    "                        words = surname.split()\n",
    "                        surname = \" \".join(words[:2])\n",
    "                        if (words[1].endswith('.') or words[1].startswith('(')):\n",
    "                            surname = words[0]\n",
    "                        person_info = df_cons[df_cons[\"surname\"] == surname].iloc[0]\n",
    "                        president_data = {\n",
    "                            \"surname\": person_info[\"surname\"],\n",
    "                            \"name\": person_info[\"name\"],\n",
    "                            \"year_birth\": person_info[\"year_birth\"],\n",
    "                            \"gender\": person_info[\"gender\"],\n",
    "                            \"group\": 'Presidente'\n",
    "                        }\n",
    "\n",
    "                chunk_idx = 1\n",
    "                for i, match in enumerate(all_matches):\n",
    "                    start_pos = match.end() \n",
    "                    end_pos = all_matches[i + 1].start() if i + 1 < len(all_matches) else len(text)\n",
    "\n",
    "                    chunk = text[start_pos:end_pos].replace(\"\\n\", \" \").strip()\n",
    "\n",
    "                    if chunk: \n",
    "                        chunk = chunk.strip()\n",
    "                        if re.match(president_pattern, match.group(0), re.IGNORECASE):\n",
    "                            if president_data:\n",
    "                                surname = president_data[\"surname\"]\n",
    "                                first_name = president_data[\"name\"]\n",
    "                                year_birth = president_data[\"year_birth\"]\n",
    "                                gender = president_data[\"gender\"]\n",
    "                                group = president_data[\"group\"]\n",
    "                            else:\n",
    "                                surname = \"Presidente\"\n",
    "                                first_name = \"N/A\"\n",
    "                                year_birth = \"N/A\"\n",
    "                                gender = \"N/A\"\n",
    "                                group = \"N/A\"\n",
    "                        else:\n",
    "                            surname = match.group(0)\n",
    "                            party = re.search(r'\\((.*?)\\)', surname)\n",
    "                            group = party.group(1) if party else \"N/A\"\n",
    "                            words = surname.split()\n",
    "                            surname = \" \".join(words[:2])\n",
    "                            if (words[1].endswith('.') or words[1].startswith('(')):\n",
    "                                surname = words[0]\n",
    "                            try:\n",
    "                                person_info = df_cons[df_cons[\"surname\"] == surname].iloc[0]\n",
    "                                first_name = person_info[\"name\"]\n",
    "                                year_birth = person_info[\"year_birth\"]\n",
    "                                gender = person_info[\"gender\"]\n",
    "                            except IndexError:\n",
    "                                first_name = \"N/A\"\n",
    "                                year_birth = \"N/A\"\n",
    "                                gender = \"N/A\"\n",
    "                                surname = \"N/A\"\n",
    "                                group = \"N/A\"\n",
    "\n",
    "                        chunk_list.append({\n",
    "                            \"ID_file\": row[\"ID_file\"],\n",
    "                            \"leg\": leg,\n",
    "                            \"class\": row[\"class\"],\n",
    "                            \"language\": language,\n",
    "                            \"surname\": surname,\n",
    "                            \"name\": first_name,\n",
    "                            \"year_birth\": year_birth,\n",
    "                            \"gender\": gender,\n",
    "                            \"group\": group,\n",
    "                            \"posizione_del_chunk\": chunk_idx,\n",
    "                            \"chunk\": chunk\n",
    "                        })\n",
    "\n",
    "                        chunk_idx += 1\n",
    "            print('File ok')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel file {row['path_clean']}: {e}. Saltando questa entry.\")\n",
    "            continue\n",
    "\n",
    "    df_chunks = pd.DataFrame(chunk_list)\n",
    "    print(f\"Numero totale di chunk processati: {len(chunk_list)}\")\n",
    "    return df_chunks\n",
    "\n",
    "df = isolate_chunk(csv_paths, csv_cons)\n",
    "\n",
    "print(\"Salvataggio del file csv_chunks.csv...\")\n",
    "df.to_csv(\"csv_chunks.csv\", index=False)\n",
    "print(\"File salvato con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione per tradurre i chunk da francese a italiano\n",
    "def translate(csv_chunks):\n",
    "    df = pd.read_csv(csv_chunks)\n",
    "    \n",
    "    filtered_df = df[df['language'] == 'fr']\n",
    "    \n",
    "    new_rows = []\n",
    "    \n",
    "    for index, row in filtered_df.iterrows():\n",
    "        translated_chunk = GoogleTranslator(source='fr', target='it').translate(row['chunk'])\n",
    "        \n",
    "        new_row = row.copy()\n",
    "        new_row['language'] = 'it'\n",
    "        new_row['chunk'] = translated_chunk  \n",
    "\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = translate(\"csv_chunks.csv\")\n",
    "\n",
    "filtered_df = df[df['language'] == 'it']\n",
    "filtered_df.to_csv(\"csv_chunks_it.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
